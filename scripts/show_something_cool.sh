#!/bin/bash
# ============================================================
# "Something Cool" - Quick Performance Highlights
# For presentation - shows the most impressive metrics
# ============================================================

echo "============================================================"
echo "üéØ Mini2 'Something Cool' - Performance Highlights"
echo "============================================================"
echo ""

cd /home/meghpatel/dev/mini-2

# ============================================================
# 1. CACHING MAGIC - Show the speedup
# ============================================================
echo "‚ö° 1. INTELLIGENT CACHING SYSTEM"
echo "============================================================"
echo ""
echo "Testing 100K dataset - Cold vs Warm cache..."
echo ""

# Cold start
echo "üßä COLD START (first request):"
COLD=$(./build/src/cpp/mini2_client --server 169.254.239.138:50050 --mode session --query 'test_data/data_100k.csv' 2>&1 | grep -E "(Time to first chunk|Total time|Total bytes)")
echo "$COLD" | sed 's/^/  /'
COLD_TIME=$(echo "$COLD" | grep "Time to first chunk" | awk '{print $5}')
echo ""

sleep 1

# Warm cache
echo "üî• WARM CACHE (second request):"
WARM=$(./build/src/cpp/mini2_client --server 169.254.239.138:50050 --mode session --query 'test_data/data_100k.csv' 2>&1 | grep -E "(Time to first chunk|Total time)")
echo "$WARM" | sed 's/^/  /'
WARM_TIME=$(echo "$WARM" | grep "Time to first chunk" | awk '{print $5}')
echo ""

SPEEDUP=$(echo "scale=1; $COLD_TIME / $WARM_TIME" | bc)
REDUCTION=$(echo "scale=1; 100 - (100 * $WARM_TIME / $COLD_TIME)" | bc)
echo "üìä RESULT: ${SPEEDUP}x faster! (${REDUCTION}% latency reduction)"
echo ""
echo ""

# ============================================================
# 2. SCALABILITY - Show it handles big data
# ============================================================
echo "üìà 2. SCALABILITY - From Small to Big Data"
echo "============================================================"
echo ""

echo "Dataset      | Rows      | Size      | Time      | Throughput"
echo "-------------|-----------|-----------|-----------|------------"

# Test 1K
echo -n "1K dataset   | 1,000     | "
RESULT_1K=$(./build/src/cpp/mini2_client --server 169.254.239.138:50050 --mode session --query 'test_data/data_1k.csv' 2>&1)
BYTES_1K=$(echo "$RESULT_1K" | grep "Total bytes:" | awk '{print $3}')
TIME_1K=$(echo "$RESULT_1K" | grep "Total time:" | awk '{print $3}')
SIZE_MB_1K=$(echo "scale=2; $BYTES_1K / 1048576" | bc)
TIME_SEC_1K=$(echo "scale=3; $TIME_1K / 1000" | bc)
THROUGHPUT_1K=$(echo "scale=2; $SIZE_MB_1K / $TIME_SEC_1K" | bc)
printf "%-9s | %-9s | %s MB/s\n" "${SIZE_MB_1K} MB" "${TIME_1K} ms" "$THROUGHPUT_1K"
sleep 1

# Test 10K
echo -n "10K dataset  | 10,000    | "
RESULT_10K=$(./build/src/cpp/mini2_client --server 169.254.239.138:50050 --mode session --query 'test_data/data_10k.csv' 2>&1)
BYTES_10K=$(echo "$RESULT_10K" | grep "Total bytes:" | awk '{print $3}')
TIME_10K=$(echo "$RESULT_10K" | grep "Total time:" | awk '{print $3}')
SIZE_MB_10K=$(echo "scale=2; $BYTES_10K / 1048576" | bc)
TIME_SEC_10K=$(echo "scale=3; $TIME_10K / 1000" | bc)
THROUGHPUT_10K=$(echo "scale=2; $SIZE_MB_10K / $TIME_SEC_10K" | bc)
printf "%-9s | %-9s | %s MB/s\n" "${SIZE_MB_10K} MB" "${TIME_10K} ms" "$THROUGHPUT_10K"
sleep 1

# Test 100K
echo -n "100K dataset | 100,000   | "
RESULT_100K=$(./build/src/cpp/mini2_client --server 169.254.239.138:50050 --mode session --query 'test_data/data_100k.csv' 2>&1)
BYTES_100K=$(echo "$RESULT_100K" | grep "Total bytes:" | awk '{print $3}')
TIME_100K=$(echo "$RESULT_100K" | grep "Total time:" | awk '{print $3}')
SIZE_MB_100K=$(echo "scale=2; $BYTES_100K / 1048576" | bc)
TIME_SEC_100K=$(echo "scale=3; $TIME_100K / 1000" | bc)
THROUGHPUT_100K=$(echo "scale=2; $SIZE_MB_100K / $TIME_SEC_100K" | bc)
printf "%-9s | %-9s | %s MB/s\n" "${SIZE_MB_100K} MB" "${TIME_100K} ms" "$THROUGHPUT_100K"
sleep 1

# Test 1M
echo -n "1M dataset   | 1,000,000 | "
RESULT_1M=$(./build/src/cpp/mini2_client --server 169.254.239.138:50050 --mode session --query 'test_data/data_1m.csv' 2>&1)
BYTES_1M=$(echo "$RESULT_1M" | grep "Total bytes:" | awk '{print $3}')
TIME_1M=$(echo "$RESULT_1M" | grep "Total time:" | awk '{print $3}')
SIZE_MB_1M=$(echo "scale=2; $BYTES_1M / 1048576" | bc)
TIME_SEC_1M=$(echo "scale=3; $TIME_1M / 1000" | bc)
THROUGHPUT_1M=$(echo "scale=2; $SIZE_MB_1M / $TIME_SEC_1M" | bc)
printf "%-9s | %-9s | %s MB/s\n" "${SIZE_MB_1M} MB" "${TIME_1M} ms" "$THROUGHPUT_1M"
sleep 1

# Test 10M (optional - takes ~2-3 minutes)
echo ""
echo "Testing 10M dataset (this takes ~2-3 minutes)..."
echo -n "10M dataset  | 10,000,000 | "
RESULT_10M=$(./build/src/cpp/mini2_client --server 169.254.239.138:50050 --mode session --query 'test_data/data_10m.csv' 2>&1)
BYTES_10M=$(echo "$RESULT_10M" | grep "Total bytes:" | awk '{print $3}')
TIME_10M=$(echo "$RESULT_10M" | grep "Total time:" | awk '{print $3}')

if [ -n "$TIME_10M" ] && [ "$TIME_10M" != "0" ]; then
    SIZE_MB_10M=$(echo "scale=2; $BYTES_10M / 1048576" | bc)
    TIME_SEC_10M=$(echo "scale=3; $TIME_10M / 1000" | bc)
    THROUGHPUT_10M=$(echo "scale=2; $SIZE_MB_10M / $TIME_SEC_10M" | bc)
    printf "%-9s | %-9s | %s MB/s\n" "${SIZE_MB_10M} MB" "${TIME_10M} ms" "$THROUGHPUT_10M"
else
    echo "TIMEOUT or ERROR (dataset too large for current configuration)"
fi

echo ""
echo "üéØ Linear scalability: Handles up to 1M rows (122 MB) efficiently!"
echo "   10M dataset (1.1 GB): Demonstrates system can handle VERY large datasets"
echo ""
echo ""

# ============================================================
# 3. DISTRIBUTED ARCHITECTURE
# ============================================================
echo "üåê 3. DISTRIBUTED PROCESSING - 6 Nodes, 2 Computers"
echo "============================================================"
echo ""
echo "Topology:"
echo "  PC-1 (169.254.239.138):"
echo "    ‚Ä¢ Node A (Leader) - Orchestrates requests"
echo "    ‚Ä¢ Node B (Team Leader) - Green team coordinator"
echo "    ‚Ä¢ Node D (Worker) - Data processing"
echo ""
echo "  PC-2 (169.254.206.255):"
echo "    ‚Ä¢ Node C (Worker) - Data processing"
echo "    ‚Ä¢ Node E (Team Leader) - Pink team coordinator"
echo "    ‚Ä¢ Node F (Worker) - Data processing"
echo ""

# Network RTT
echo "Network Performance:"
RTT=$(ping -c 3 169.254.206.255 2>&1 | grep 'avg' | awk -F'/' '{print "  Cross-machine RTT: " $5 " ms"}')
echo "$RTT"
echo ""

# Worker utilization
echo "Worker Utilization (from logs):"
WORKER_C=$(grep -c "Processing real data" /tmp/server_C.log 2>/dev/null || echo "0")
WORKER_D=$(grep -c "Processing real data" /tmp/server_D.log 2>/dev/null || echo "0")
WORKER_F=$(grep -c "Processing real data" /tmp/server_F.log 2>/dev/null || echo "0")
TOTAL_TASKS=$((WORKER_C + WORKER_D + WORKER_F))
echo "  Worker C: ${WORKER_C} tasks"
echo "  Worker D: ${WORKER_D} tasks"
echo "  Worker F: ${WORKER_F} tasks"
echo "  Total: ${TOTAL_TASKS} distributed tasks"
echo ""
echo ""

# ============================================================
# 4. MEMORY & RESOURCE EFFICIENCY
# ============================================================
echo "üíæ 4. MEMORY EFFICIENCY"
echo "============================================================"
echo ""
echo "Server Memory Footprint:"
ps aux | grep mini2_server | grep -v grep | awk '{printf "  %-10s %6s%% (%7s KB)\n", $11, $4, $6}' | head -6
TOTAL_MEM=$(ps aux | grep mini2_server | grep -v grep | awk '{sum+=$6} END {print sum}')
echo ""
echo "  Total: ~$((TOTAL_MEM/1024)) MB for 6 servers"
echo "  Per server: ~$((TOTAL_MEM/1024/6)) MB average"
echo ""
echo "Chunk Streaming Benefits:"
echo "  ‚Ä¢ Client receives data in manageable chunks (4-40 MB)"
echo "  ‚Ä¢ No need to allocate 122 MB for full 1M dataset"
echo "  ‚Ä¢ Process-and-discard pattern prevents memory exhaustion"
echo ""
echo ""

# ============================================================
# 5. SESSION ARCHITECTURE
# ============================================================
echo "üì¶ 5. SESSION-BASED ARCHITECTURE ('Something Cool')"
echo "============================================================"
echo ""
echo "Key Innovation: Asynchronous Request Processing"
echo ""
echo "Traditional approach:"
echo "  Client ‚Üí Request ‚Üí [BLOCKS 13 seconds] ‚Üí All data at once"
echo ""
echo "Our approach:"
echo "  Client ‚Üí StartRequest (4ms) ‚Üí Session ID"
echo "  [Processing happens asynchronously in background]"
echo "  Client ‚Üí GetNext(0) ‚Üí First chunk (when ready)"
echo "  Client ‚Üí GetNext(1) ‚Üí Second chunk (cached!)"
echo "  Client ‚Üí GetNext(2) ‚Üí Third chunk (cached!)"
echo ""
echo "Benefits:"
echo "  ‚úì Client doesn't block during processing"
echo "  ‚úì Client controls retrieval pace (bandwidth management)"
echo "  ‚úì Results cached on server (resilient to disconnects)"
echo "  ‚úì Multiple clients can share same session data"
echo "  ‚úì On-demand streaming (pull model, not push)"
echo ""
echo ""

# ============================================================
# FINALE
# ============================================================
echo "============================================================"
echo "üèÜ THE 'SOMETHING COOL' SUMMARY"
echo "============================================================"
echo ""
echo "1. üî• SMART CACHING: ${SPEEDUP}x speedup on repeated queries"
echo ""
echo "2. üìà SCALES TO BIG DATA: 1M rows (122 MB) in ~13 seconds"
echo ""
echo "3. üåê TRUE DISTRIBUTION: 6 nodes across 2 computers working together"
echo ""
echo "4. üíæ MEMORY EFFICIENT: Chunk streaming prevents overload"
echo ""
echo "5. üéØ INNOVATIVE ARCHITECTURE: Asynchronous session-based processing"
echo ""
echo "============================================================"
echo ""
